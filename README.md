# ğŸ›¡ï¸ Content Moderation System (Text, Image, Video)

A Flask-based web application for automatic moderation of **text**, **images**, and **videos**. The system classifies content into:
- âœ… Safe
- âš ï¸ Potentially Harmful
- âŒ Prohibited

It uses pre-trained models and NLP/computer vision techniques to detect nudity, violence, hate symbols, toxic language, and more.

---

## ğŸ”§ Tech Stack

- **Flask** â€“ Web framework
- **Torch + Transformers** â€“ CLIP model for image classification
- **OpenCV** â€“ Video frame extraction
- **scikit-learn + NLTK** â€“ Text moderation using TF-IDF and Random Forest
- **HTML + CSS** â€“ Frontend UI (with a responsive layout)

---

## ğŸš€ Features

- âœ… Upload and classify text, image, or video content
- âœ… Returns category and confidence level
- âœ… Clean UI with Bootstrap/CSS
- âœ… Image & text pre-trained model support

---

## ğŸ“‚ Project Structure

content_moderation_system/
â”œâ”€â”€ app.py
â”œâ”€â”€ classify_image.py
â”œâ”€â”€ text_moderation_response.py
â”œâ”€â”€ video_classification.py
â”œâ”€â”€ txt_training.py                    # Uses final_dataset.csv
â”œâ”€â”€ final_dataset.csv                  # CSV used for model training
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ traditional_moderation_model.pkl
â”‚   â”œâ”€â”€ image_model.pkl
â”‚   â””â”€â”€ video_model.pkl
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Procfile                          
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore                       
â””â”€â”€ venv/                    

--

## ğŸ” How It Works

This project automatically moderates user-generated text, image, and video content using trained machine learning and deep learning models. Here's how each type of content is handled:

ğŸ“ Text Moderation
- âœ… Preprocessing: Cleans and tokenizes user input using NLTK (stopword removal, lemmatization).
- âœ… Feature Extraction: Converts text into TF-IDF vectors.
- âœ… Classification: Predicts label using a trained RandomForestClassifier.
- âœ… Output: Displays moderation label such as Safe, Hate Speech, or Abusive.

ğŸ–¼ï¸ Image Moderation
- âœ… Model: Uses OpenAIâ€™s pre-trained CLIP model.
- âœ… Process: Compares uploaded image with moderation categories (e.g., nudity, violence, hate symbols).
- âœ… Confidence Check: Applies a threshold (e.g., 60%) to determine label.
- âœ… Output: Displays the most relevant label and confidence score.

ğŸï¸ Video Moderation
- âœ… Frame Extraction: Uses OpenCV to extract frames from the uploaded video.
- âœ… Per-Frame Classification: Each frame is analyzed using the image moderation pipeline.
- âœ… Aggregation: Final result is based on the most frequent frame label.
- âœ… Output: Returns the dominant classification for the video.


